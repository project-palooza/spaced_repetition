{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "p = 0.001  # 1% of the lines\n",
    "# keep the header, then take only 10% of lines\n",
    "# if random from [0,1] interval is greater than 0.1 the row will be skipped\n",
    "df = pd.read_csv(\n",
    "         'learning_traces.13m.csv',\n",
    "         header=0, \n",
    "         skiprows=lambda i: i>0 and random.random() > p\n",
    ")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'],unit='s')\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we try to learn good values for theta \n",
    "# we need to construct x\n",
    "\n",
    "# x = information a students history learning a certain word\n",
    "\n",
    "df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df['p_recall'] == (df['session_correct'])/(df['session_seen'])).sum() == df.shape[0]\n",
    "\n",
    "# p_recall is the ratio of session_correct/session_seen\n",
    "\n",
    "# p_recall is \"y\" \"ground truth\"\n",
    "\n",
    "# predicted_p_recall is \"y_hat\" \"prediction\"\n",
    "\n",
    "# error(p_recall,predicted_p_recall) <- we want this to be as small as possible\n",
    "\n",
    "# if we can very reliably predict p_recall, what is the value of this in real-life terms?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parts of speech\n",
    "\n",
    "def lexeme_df(filename):\n",
    "\n",
    "    import re\n",
    "    df_single_col = pd.read_csv(filename, delimiter='\\t', header=None, names=['line'])\n",
    "\n",
    "    def split_line(line):\n",
    "        parts = re.split(r'\\s+', line, maxsplit=2)\n",
    "        if len(parts) == 3:\n",
    "            return parts\n",
    "        return [None, None, None]\n",
    "\n",
    "    df_split = df_single_col['line'].apply(split_line)\n",
    "    df = pd.DataFrame(df_split.tolist(), columns=['lexeme', 'category', 'meaning'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexemes = lexeme_df('lexeme_reference.txt')\n",
    "lexemes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexemes['lexeme'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "# dummy variables / indicator variables\n",
    "\n",
    "# df.loc[0,\"lexeme_string\"]\n",
    "\n",
    "\"<det><def><nt><sg><nom>\"\n",
    "\n",
    "# det, df, nt, sg, nom + 87 more \n",
    "\n",
    "# word | det | def | nt | sg | nom | ...\n",
    "# das  |  1  | 1   | 1  | 1  |  1  | 0 ...\n",
    "\n",
    "\n",
    "\n",
    "# look for the first <, remove everything to the left\n",
    "# then remove <, >\n",
    "\n",
    "# \n",
    "\n",
    "def extract_right_of_lt(text):\n",
    "    import re\n",
    "    match = re.search(r'<(.*)', text)\n",
    "    return match.group(1) if match else ''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lexeme_string'] = df['lexeme_string'].apply(extract_right_of_lt)\n",
    "df['lexeme_string'] = df['lexeme_string'].str.replace(\"<\",\" \")\n",
    "df['lexeme_string'] = df['lexeme_string'].str.replace(\">\",\"\")\n",
    "df['lexeme_string'] = df['lexeme_string'].str.replace(\"*\",\"\")\n",
    "df['lexeme_string'] = df['lexeme_string'].str.replace(\"/\",\"\")\n",
    "df['lexeme_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(), binary=True)\n",
    "vectorized_words = vectorizer.fit_transform(df['lexeme_string'])\n",
    "vectorized_df = pd.DataFrame(vectorized_words.toarray(), columns=vectorizer.get_feature_names_out(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no longer optimizing this for now,\n",
    "# i want to talk about specifying the model\n",
    "# we'll return to this\n",
    "\n",
    "vectorized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df.sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curious about most common parts of speech.\n",
    "# how different parts of speech correspond to rate of correct responses\n",
    "\n",
    "#\n",
    "vectorized_df.sum().sort_values(ascending=False)[0:20].plot(kind = \"bar\",title=\"most common parts of speech\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_recall \n",
    "\n",
    "df = pd.concat([df,vectorized_df],axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['lexeme_string'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_by_columns = list(df.columns[~df.columns.isin(['p_recall','delta','user_id','timestamp',\"history_seen\", \"history_correct\", \"session_seen\",  \"session_correct\"])])\n",
    "\n",
    "\"\"\"\n",
    "select avg(p_recall)\n",
    "from df\n",
    "group by var1\n",
    "\n",
    "select avg(p_recall)\n",
    "from df\n",
    "group by var2\n",
    "\n",
    "...\n",
    "\n",
    "select avg(p_recall)\n",
    "from df\n",
    "group by varn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "select avg(p_recall)\n",
    "from df\n",
    "group by var1, ..., varn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 1515 binary vectors (one for each part of speech)\n",
    "\n",
    "# 0 avg(p_recall)\n",
    "# 1 avg(p_recall)\n",
    "\n",
    "# df.drop(columns = grp_by_columns).mean(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# df[grp_by_columns].mean(axis = 1)\n",
    "\n",
    "list_of_recall_variation_by_column = []\n",
    "\n",
    "for col in grp_by_columns:\n",
    "\n",
    "    variation_of_means = df.groupby(col)['p_recall'].mean().std()\n",
    "\n",
    "    variation_dict = {\"column_name\":col,\n",
    "                      \"variation_of_means\":variation_of_means}\n",
    "\n",
    "    list_of_recall_variation_by_column.append(variation_dict)\n",
    "\n",
    "recall_variation_by_column = pd.DataFrame(list_of_recall_variation_by_column)\n",
    "    \n",
    "# col_name, std of group means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should start with a really simple feature vector\n",
    "# (history_seen, history_correct)\n",
    "\n",
    "# decision: recode delta to days (that's what duolingo did)\n",
    "# round to one decimal point\n",
    "\n",
    "simple_df = df[['p_recall','delta','history_seen','history_correct']]\n",
    "simple_df[\"delta_days\"] = np.round(simple_df.loc[:,\"delta\"].copy()/(60*60*24),1)\n",
    "simple_df.drop(columns = [\"delta\"],inplace=True)\n",
    "simple_df.loc[simple_df[\"p_recall\"] == 0,\"p_recall\"] = simple_df.loc[simple_df[\"p_recall\"] == 0,\"p_recall\"] + 1e-3\n",
    "\n",
    "# h\n",
    "# -delta/log2(p)\n",
    "simple_df[\"h\"] = -1*simple_df[\"delta_days\"]/(np.log2(simple_df[\"p_recall\"]) + 1e-3)\n",
    "\n",
    "simple_df.sample(5)\n",
    "# input_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_hat = 2**(theta*x)\n",
    "\n",
    "# theta = [1,3]\n",
    "# x = [10,5]\n",
    "\n",
    "# 10 + 15 = 25\n",
    "\n",
    "# h_hat = 2**25\n",
    "# predicted_p_recall = 2**(-1*(2/(2**25))) = 1\n",
    "\n",
    "# we need an error function to quantify how wrong we are\n",
    "\n",
    "# (p_recall - predicted_p_recall)**2 <- error\n",
    "# predicted_p_recall = 2**(-1*(delta/predicted_half_life))\n",
    "# predicted_half_life = 2**(theta*x)\n",
    "\n",
    "\n",
    "# given\n",
    "# p_recall is given\n",
    "# delta is given\n",
    "# x is given\n",
    "\n",
    "# we don't have theta\n",
    "\n",
    "# we are going initialize theta with some random numbers\n",
    "# so then we have theta\n",
    "\n",
    "# once we have theta we can calculate the error\n",
    "# and start learning\n",
    "\n",
    "# to-do\n",
    "\n",
    "# write formulas for predicted half_life and predicted p_recall\n",
    "# write formula for loss function\n",
    "# and import an optimizer \n",
    "# run the optimizer on the loss function + our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to use pytorch\n",
    "# we are using custom loss function and our model is not one of the standard ML models, like linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fresh instance of SpacedRepetition model\n",
    "# what happens \n",
    "# mymodel = SpacedRepetition(2)\n",
    "# under the hood\n",
    "# theta = [theta1,theta2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacedRepetition(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, alpha, lambda_reg):\n",
    "\n",
    "        super(SpacedRepetition, self).__init__()\n",
    "        self.theta = nn.Linear(input_dim, 1, bias=False)\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, x):\n",
    "        # estimating h_hat = 2^(theta . x)\n",
    "        theta_x = self.theta(x)  # dot product of theta and x\n",
    "        h_hat = torch.pow(2, theta_x)\n",
    "        return h_hat\n",
    "\n",
    "    def prediction(self, h_hat, delta):\n",
    "        p_hat = torch.pow(2, -1 * (delta / h_hat))\n",
    "        return p_hat\n",
    "\n",
    "    def loss(self,p, p_hat, h, h_hat):\n",
    "        loss_p = torch.sum((p_hat - p) ** 2)\n",
    "        loss_h = torch.sum((h_hat - h) ** 2)\n",
    "        reg_term = self.lambda_reg * torch.sum(self.theta.weight ** 2)\n",
    "        total_loss = loss_p + self.alpha * loss_h + reg_term\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object oriented programming\n",
    "\n",
    "# python is a very flexible programming language\n",
    "\n",
    "# one thing it lets you do is define things called Classes\n",
    "\n",
    "# suppose you are a game developer\n",
    "# designing a world for your game\n",
    "# and your world has trees\n",
    "# you write a tree class\n",
    "# tree class defines what attributes trees can have\n",
    "# tree: height, color, bears_fruit, number of leaves, ...\n",
    "\n",
    "# tree_A = tree(height = 100, color = green, bears_fruit = false, number of leaves = 1800)\n",
    "\n",
    "# tree class exists\n",
    "# new class called MagicTrees\n",
    "\n",
    "# class MagicTree(Tree):\n",
    "# super(MagicTree, self).__init__()\n",
    "\n",
    "# MagicTree is called a sub-class of Tree\n",
    "# Tree is a superclass of MagicTree\n",
    "\n",
    "# linear regression is implemented as a class\n",
    "# result = smf.ols(\"y ~ x\",data=df)\n",
    "# result.summary\n",
    "# result.params\n",
    "# ....\n",
    "\n",
    "# class SpacedRepetition(nn.Module)\n",
    "# super(SpacedRepetition, self).__init__()\n",
    "\n",
    "# inheritance\n",
    "\n",
    "\n",
    "# __init__(self, input_dim) \"dunder method\" \"double underscore method\"\n",
    "\n",
    "# mymodel = SpacedRepetition(2)\n",
    "# np.dot(mymodel.theta.weight[0].detach().numpy(),simple_df[['history_seen','history_correct']].iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .317\n",
    "\n",
    "# 2**(.317)\n",
    "# predicted_half_life = 1.24\n",
    "# predicted_p_recall = 2**(-1*(83/1.24)) = 0\n",
    "# real p_recall = 1\n",
    "\n",
    "# (real p_recall - predicted p recall)**2 = 1\n",
    "\n",
    "# after optimization\n",
    "# hopefully we have thetas that produce a predicted p recall that is closer to the truth\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# 128,000 records\n",
    "# each has history_seen, history_correct\n",
    "# x_i = [history_seen, history_correct] for i in range(128000)\n",
    "# y_i = p recall for ith person\n",
    "# using theta and x_i we take a dot product theta*x_i = z_i\n",
    "# predicted_hat_i = 2**(z_i)\n",
    "# delta is the last time (in days) that they saw the word\n",
    "# predicted_p_recall_i = 2**(-1*(delta/predicted_hat_i))\n",
    "# predicted_p_recall_i is our \"final output\"\n",
    "# now we do it for i = 1, ..., i = 128,000\n",
    "# measure how wrong we are\n",
    "# (p_recall_i - predicted_p_recall_i)**2 <- there's more to the error function but this what makes it go\n",
    "# add it up for i = 1 , ..., i = 128,000\n",
    "# ???????? optimization\n",
    "# we have a new theta - new theta will hopefully yield smaller error\n",
    "# rinse and repeat until error stops going down.\n",
    "# then you have your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half_life = 2**(theta*x)\n",
    "#\n",
    "# simple_df[['history_seen','history_correct']].iloc[0,:]\n",
    "# df.loc[0,\"delta\"]/60/60/24\n",
    "# df.loc[0,'p_recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "# reminder of the training process\n",
    "\n",
    "# step 1: we pass all of our data through the model to get predicted h_hat\n",
    "# we will have 129,228 predictions for h_hat\n",
    "# will use these h_hats to get 129,228 predictions for p_recall\n",
    "# we will check these predicted p_recalls against observed p_recall (predicted_p_recall - p_recall)**2\n",
    "# we add up these squared errors across all 129,228 rows\n",
    "# then we update model parameters (we will use a in-built or provided optimizer) (we will not get into the details)\n",
    "# then we start again at the top, at step 1:\n",
    "# we do this many times, until our error no longer improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_torch_vector(vec):\n",
    "    mean = vec.mean(dim=0, keepdim=True)\n",
    "    std = vec.std(dim=0, keepdim=True)\n",
    "    vec_standardized = (vec - mean) / std\n",
    "    return vec_standardized\n",
    "\n",
    "def minmax_torch_vector(vec):\n",
    "    min_ = vec.min(dim=0)\n",
    "    max_ = vec.max(dim=0)\n",
    "    vec_minmax = (vec - min_.values)/(max_.values - min_.values)\n",
    "    return vec_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100 \n",
    "alpha = 0.01\n",
    "lambda_reg = 0.01\n",
    "\n",
    "model = SpacedRepetition(input_dim, alpha, lambda_reg)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "x = minmax_torch_vector(torch.tensor(simple_df[['history_seen', 'history_correct']].values, dtype=torch.float32))\n",
    "p = minmax_torch_vector(torch.tensor(simple_df['p_recall'].values, dtype=torch.float32))\n",
    "h = minmax_torch_vector(torch.tensor(simple_df['h'].values, dtype=torch.float32))\n",
    "delta = minmax_torch_vector(torch.tensor(simple_df['delta_days'].values, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    h_hat = model.forward(x)\n",
    "    p_hat = model.prediction(h_hat, delta)\n",
    "\n",
    "    loss = model.loss(p, p_hat, h, h_hat)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_epochs), losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    h_hat = model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sr",
   "language": "python",
   "name": "sr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
